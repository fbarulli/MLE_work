{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting Classifier\n",
    "The Voting Classifier is a scikit-learn meta-classifier for combining several similar or conceptually different Machine Learning estimators. Specifically, it consists of assembling a college of experts, represented by models such as decision trees,  ùëò\n",
    "  nearest neighbors or logistic regression, and then putting them to the vote.\n",
    "Scikit-learn's VotingClassifier class enables you to carry out a hard or soft vote.\n",
    "\n",
    "In 'hard' voting, each classification model predicts a label, and the final label produced is the one predicted most frequently.\n",
    "\n",
    "In 'soft' voting, each model returns a probability for each class, and the probabilities are averaged to predict the final class (only recommended if classifiers are well calibrated).\n",
    "\n",
    "In both cases, you can assign a weight to each estimator, allowing you to give more weight to one or more models.\n",
    "\n",
    "Stacking\n",
    "Stacking is an ensemble method wherein the principle involves simultaneously training various Machine Learning algorithms, whose results are then used to train a new model that optimally combines the predictions of the initial estimators.\n",
    "This method relies on the following technique :\n",
    "\n",
    "The first step involves specifying a list of L base algorithms and their corresponding parameters, as well as the meta-learning algorithm.\n",
    "Each of the L algorithms is then trained on the training set, containing N observations.\n",
    "Cross-validation is used to obtain predictions from each of the models for the N observations.\n",
    "The meta-learning algorithm is then trained on this collected data and makes new predictions.\n",
    "The ensemble model ultimately consists of the set of L base algorithms and the meta-learning model and can be used to generate predictions on a new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_validate\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"mathchi/diabetes-data-set\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "directory = '/Users/notagain/.cache/kagglehub/datasets/mathchi/diabetes-data-set/versions/1'\n",
    "\n",
    "df = pd.read_csv(os.path.join(directory, 'diabetes.csv'))  \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KNN]: \n",
      " Accuracy: 0.71 (+/- 0.02) F1 score: 0.58 (+/- 0.02)\n",
      "[Random Forest]: \n",
      " Accuracy: 0.76 (+/- 0.03) F1 score: 0.63 (+/- 0.04)\n",
      "[Logistic Regression]: \n",
      " Accuracy: 0.76 (+/- 0.02) F1 score: 0.61 (+/- 0.05)\n",
      "[Voting Classifier]: \n",
      " Accuracy: 0.77 (+/- 0.00) F1 score: 0.64 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "data = df.drop('Outcome', axis=1)\n",
    "target = df['Outcome']\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=4)\n",
    "clf1 = KNeighborsClassifier(n_neighbors=3)\n",
    "clf2 = RandomForestClassifier(random_state=123)\n",
    "clf3 = LogisticRegression(max_iter=1000)\n",
    "vclf = VotingClassifier(estimators=[('knn', clf1), ('rf', clf2), ('lr', clf3)], voting='hard')\n",
    "cv3 = KFold(n_splits=3, random_state=111, shuffle=True)\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, vclf], ['KNN', 'Random Forest', 'Logistic Regression', 'Voting Classifier']):\n",
    "    scores = cross_validate(clf, X_train, y_train, cv=cv3, scoring=['accuracy','f1'])\n",
    "    print(\"[%s]: \\n Accuracy: %0.2f (+/- %0.2f)\" % (label, scores['test_accuracy'].mean(), scores['test_accuracy'].std()),\n",
    "          \"F1 score: %0.2f (+/- %0.2f)\" % (scores['test_f1'].mean(), scores['test_f1'].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estimators': [('knn', KNeighborsClassifier(n_neighbors=3)), ('lr', LogisticRegression(max_iter=1000))]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'estimators': [[('knn', clf1), ('lr', clf3)], \n",
    "                  [('knn', clf1), ('rf', clf2)]]\n",
    "}\n",
    "grid = GridSearchCV(estimator=vclf, param_grid=params, cv=5)\n",
    "grid = grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StackingClassifier]: \n",
      " Accuracy: 0.76 (+/- 0.02)\n",
      " F1 score: 0.62 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "sclf = StackingClassifier(estimators=[('knn', clf1), ('rf', clf2), ('lr', clf3)], final_estimator=clf3)\n",
    "\n",
    "scores = cross_validate(sclf, X_train, y_train, cv=cv3, scoring=['accuracy', 'f1'])\n",
    "    \n",
    "print(\"[StackingClassifier]: \\n Accuracy: %0.2f (+/- %0.2f)\\n\" % (scores['test_accuracy'].mean(), scores['test_accuracy'].std()),\n",
    "      \"F1 score: %0.2f (+/- %0.2f)\" % (scores['test_f1'].mean(), scores['test_f1'].std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc : 0.8138528138528138\n",
      "Acc : 0.8008658008658008\n"
     ]
    }
   ],
   "source": [
    "vclf.fit(X_train, y_train)\n",
    "sclf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Acc :\", vclf.score(X_test, y_test))\n",
    "print(\"Acc :\", sclf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc : 0.7878787878787878\n"
     ]
    }
   ],
   "source": [
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "print(\"Acc :\", clf2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we saw two overall methods: Voting Classifier and Stacking. Unlike Bagging or Boosting, these methods aim to combine several already efficient estimators to benefit from their specific advantages.\n",
    "\n",
    "The Voting Classifier is a meta-classifier from scikit-learn which allows you to combine different Machine Learning models, whether similar or conceptually different. It can perform a 'hard' vote, where each model predicts a label and the final label is determined by the majority vote, or a 'soft' vote, where the probabilities predicted by each model are averaged.\n",
    "\n",
    "Stacking is an ensemble method that trains multiple machine learning algorithms simultaneously, then uses their predictions to train a new model. This method involves specifying a list of base algorithms and a meta-learning algorithm, then training each algorithm on a dataset and using cross-validation to obtain the predictions. Finally, the meta-learning model is trained on these predictions.\n",
    "\n",
    "In the case of very large databases, and if the calculation time should not be too long, these methods are not necessarily preferred. Some algorithms use boosting/bagging methods in an optimized manner to obtain solid performance while reducing their calculation time, this is the case for example of XGBoost, which you can discover in the following notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
